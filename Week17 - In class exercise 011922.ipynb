{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a2fdfb",
   "metadata": {},
   "source": [
    "# In class exercise\n",
    "### 01/19/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378ac83",
   "metadata": {},
   "source": [
    "**1. Write simple (straightforward) definitions for the following parameters for RandomForestClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and indicate how they correlate with the precision and recall for the basic diabetes model we built in class. You will need to rerun the model multiple times to do\n",
    "so.**\n",
    "\n",
    "The table below is a super basic summary. More details follow in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e519b4f",
   "metadata": {},
   "source": [
    "Parameter|Definition |Correlation with Precision |Correlation with Recall\n",
    ":-----|:-----|:-----|:----- \n",
    "estimators|Number of decision trees|Back and forth by 1|Back and forth by 1\n",
    "max_depth|Maximum levels below the root node of your decision trees |Negative|Positive\n",
    "min_samples_split|Minimum samples to take before a node can split|Slight positive|slight positive\n",
    "min_samples_leaf|Guarantees minimum number of samples in a leaf node|Positive|Negative\n",
    "min_weight_fraction_leaf|fraction of input samples required to be at a leaf node|Positive|Negative\n",
    "max_leaf_nodes|The maximum number of leaf nodes in your trees|Negative|Positive\n",
    "min_impurity_decrease|The node will be split if doing so will decrease the impurity|Positive|Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e2cc0",
   "metadata": {},
   "source": [
    "Set up the training/testing data from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc72dd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load in data frame\n",
    "diabetes_df = pd.read_csv('diabetes.csv')\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9dd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1)\n",
    "y = diabetes_df['Outcome']\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "#Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72472c2",
   "metadata": {},
   "source": [
    "Import the RandomForestClassifier from class. Then, fit the model to a variety of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e442a2",
   "metadata": {},
   "source": [
    "**n_estimators: Number of decision trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee62571",
   "metadata": {},
   "source": [
    "I tried estimators of 50, 100, 200, 300, 500, and 700. Surprisingly, the precision/recall barely changed at all. It wavered only 1 or 2 points for both. The best combination was at 500, which is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4868a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       150\n",
      "           1       0.71      0.58      0.64        81\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.75      0.73      0.74       231\n",
      "weighted avg       0.77      0.77      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the classifier\n",
    "est_rf = est_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = est_rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0466a",
   "metadata": {},
   "source": [
    "**Max_Depth: Maximum levels below the root node of your decision trees**\n",
    "\n",
    "I tested 1-6, 10, 15, and 20. The lower the number, the better the precision. The higher the number, the better the recall (although, this changed once the depth got larger than 10). 10 was the overall best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b0edeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       150\n",
      "           1       0.69      0.58      0.63        81\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.72      0.73       231\n",
      "weighted avg       0.76      0.76      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "md_rf = RandomForestClassifier(max_depth=10, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "md_rf = md_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = md_rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f60dc0",
   "metadata": {},
   "source": [
    "**Min_samples_split: Minimum samples to take before a node can split**\n",
    "\n",
    "Decision and recall increase/decrease by 1 or 2 points each. As I increased the min_samples, precision increased modestly. Recall increased by 1 with each increment until 4, which is where it peaked. The best overall value was 4 (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0003650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       150\n",
      "           1       0.68      0.58      0.63        81\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.72      0.72       231\n",
      "weighted avg       0.75      0.76      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mss_rf = RandomForestClassifier(min_samples_split=4, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "mss_rf = mss_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = mss_rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa790f2",
   "metadata": {},
   "source": [
    "**Min_samples_leaf: Guarantees minimum number of samples in a leaf node**\n",
    "\n",
    "Again, the values waver only slightly for all combinations. I tested 1-4, and then 10, 20, 50, 100, and 125. 125 gave me a precision of 100 and a recall of 7! In general, as I increased this parameter, the precision went up and recall went down (roughly). The best overall value I found was for min_samples_leaf=2, which is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e70fb283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.83       150\n",
      "           1       0.70      0.56      0.62        81\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.71      0.72       231\n",
      "weighted avg       0.76      0.76      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msl_rf = RandomForestClassifier(min_samples_leaf=2, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "msl_rf = msl_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = msl_rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94841779",
   "metadata": {},
   "source": [
    "**Min_weight_fraction_leaf: fraction of input samples required to be at a leaf node**\n",
    "\n",
    "In general, precision increased and recall decreased as I increased this parameter. The default of 0 had the best recall (displayed below). Between 0 and 0.1, recall decreased from 54 to 47 and then had more modest decreases afterwards. However, precision increased by about 2 points per parameter change. The best overall was the default value of 0 (displayed below).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ab315ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       150\n",
      "           1       0.68      0.54      0.60        81\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.70      0.71       231\n",
      "weighted avg       0.74      0.75      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mwf_rf = RandomForestClassifier(min_weight_fraction_leaf=0, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "mwf_rf = mwf_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = mwf_rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bedacc4",
   "metadata": {},
   "source": [
    "**Max_leaf_nodes: The maximum number of leaf nodes in your trees**\n",
    "\n",
    "Overall, the predicsion would decrease by 1 or 2 points as I increased the parameter. However, recall would increase by a few points. Despite this relationship, the best value was actually when I set the parameter to \"None\" (below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a72f6c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       150\n",
      "           1       0.68      0.54      0.60        81\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.70      0.71       231\n",
      "weighted avg       0.74      0.75      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mln_rf = RandomForestClassifier(max_leaf_nodes=None, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "mln_rf = mln_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = mln_rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1746f",
   "metadata": {},
   "source": [
    "**min_impurity_decrease: The node will be split if doing so will decrease the impurity**\n",
    "\n",
    "This parameter worked best when I tested it in increments of 0.001. As I increased the parameter, the precision increased by 1-2 points, while the recall lowered by 1-2 points. The largest extreme was a value of 0.04, which resulted in a precision of 80 and a recall of 25. The best overall was a weight of 0.003, which is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c5b6105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       150\n",
      "           1       0.71      0.57      0.63        81\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.75      0.72      0.73       231\n",
      "weighted avg       0.76      0.77      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mid_rf = RandomForestClassifier(min_impurity_decrease=0.003, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "mid_rf = mid_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = mid_rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a6765",
   "metadata": {},
   "source": [
    "**2. How does setting bootstrap=False influence the model performance? Note: the default is bootstrap=True. Explain why your results might be so.**\n",
    "\n",
    "Using the in-class example as a reference...you can see that when boot strap is set to true, the model is ever so slightly worse. When set to \"false,\" the precision increases by 1 and the recall increases by 1 (however, the overall  accuracy stays the same). This is likely because instead of pooling different samples in our data set, the model is using the entire data set for each tree. This would mean each tree has more information to work with. Practically speaking though, setting bootstrap to \"false,\" is less computationally efficient than if we were to keep it as true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe1192cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       150\n",
      "           1       0.70      0.58      0.64        81\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.75      0.72      0.73       231\n",
      "weighted avg       0.76      0.77      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#estimator = number of decision trees\n",
    "rf = RandomForestClassifier(n_estimators=200, bootstrap=True, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "rf = rf.fit(X_train, y_train)\n",
    "# Get the accuracy score\n",
    "rf.score(X_test, y_test)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "befa45e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       150\n",
      "           1       0.71      0.59      0.64        81\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.75      0.73      0.74       231\n",
      "weighted avg       0.77      0.77      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#estimator = number of decision trees\n",
    "rf = RandomForestClassifier(n_estimators=200, bootstrap=False, random_state=42)\n",
    "\n",
    "# Fit the classifier\n",
    "rf = rf.fit(X_train, y_train)\n",
    "# Get the accuracy score\n",
    "rf.score(X_test, y_test)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
