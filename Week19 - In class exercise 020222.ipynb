{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc81d48",
   "metadata": {},
   "source": [
    "# Week 19: In class exercise\n",
    "## Unsupervised Learning\n",
    "### 02/02/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b22fa1",
   "metadata": {},
   "source": [
    "**1. Take one of the supervised learning models you have built recently and apply at least three dimensionality reduction techniques to it (separately). Be sure to create a short summary of each technique you use. Indicate how each changed the model performance. Reference:** https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0b978",
   "metadata": {},
   "source": [
    "First, load in the necessary libraries and read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a145018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>76</td>\n",
       "      <td>27</td>\n",
       "      <td>200</td>\n",
       "      <td>35.9</td>\n",
       "      <td>0.483</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.587</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>0.190</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>74</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>33.2</td>\n",
       "      <td>0.591</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.284</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "700            2      122             76             27      200  35.9   \n",
       "62             5       44             62              0        0  25.0   \n",
       "583            8      100             76              0        0  38.7   \n",
       "685            2      129             74             26      205  33.2   \n",
       "735            4       95             60             32        0  35.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "700                     0.483   26        0  \n",
       "62                      0.587   36        0  \n",
       "583                     0.190   42        0  \n",
       "685                     0.591   25        0  \n",
       "735                     0.284   28        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load in file\n",
    "diabetes_df = pd.read_csv(\"diabetes.csv\")\n",
    "# View sample\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fcb13",
   "metadata": {},
   "source": [
    "**Original Logistic Regression without any tuning or dimensionality reduction**\n",
    "\n",
    "Without applying any additional techniques, the logistic regression model produces a recall score of 52% and an accuracy of 75%. Our overall accuracy is okay, but the recall is definitely suffering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af478fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into outcome vs predictors\n",
    "X = diabetes_df.drop('Outcome', axis=1)\n",
    "y = diabetes_df['Outcome']\n",
    "\n",
    "# Split into training vs. testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33, stratify=y)\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ba4004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       150\n",
      "           1       0.70      0.52      0.60        81\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.74      0.70      0.71       231\n",
      "weighted avg       0.75      0.75      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up classifier\n",
    "log = LogisticRegression(random_state=33)\n",
    "\n",
    "# Train our logistic regression model\n",
    "log_model = log.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted values\n",
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "# View model performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9781be3",
   "metadata": {},
   "source": [
    "**First technique: Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA is a dimensionality reduction technique that plots data from higher dimensions onto lower dimensions. In the process of doing a PCA, we are finding dimensions that are possibly correlated with one another. We then group those into sets of data points that are linearly *uncorrelated* in order to maximize the amount of variance/information for our analysis. Those values are the 'principal components (PCs).' We can then learn the total amount of variance (or information) that our principal components are holding. We can narrow our components to only those that hold a substantial amount of variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad3c2a",
   "metadata": {},
   "source": [
    "First, we have to select our number of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1274c371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 8 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create scaler instance\n",
    "scaler = StandardScaler()\n",
    "# Create PCA instance\n",
    "pca = PCA()\n",
    "\n",
    "# Make pipeline to apply scaler and pca\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to our original predictor values\n",
    "pipeline.fit(X.values)\n",
    "\n",
    "# View the full range of possible components\n",
    "features = range(pca.n_components_)\n",
    "# Plot the features against their explained variance\n",
    "plt.bar(features, pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbca984",
   "metadata": {},
   "source": [
    "By looking at this, it appears there are two features that hold the most variance collectively. However, if we only had those two features, a lot of information would be lost in our model. We can see that the explained variance is distributed rather evenly once we get to the 3rd component. There is a bit of a drop off after the 6th, so I will make the number of components 6 in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4fea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.86      0.79       150\n",
      "           1       0.61      0.41      0.49        81\n",
      "\n",
      "    accuracy                           0.70       231\n",
      "   macro avg       0.67      0.63      0.64       231\n",
      "weighted avg       0.69      0.70      0.68       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=33, stratify=y)\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.fit_transform(X_test)\n",
    "\n",
    "# Create the PCA instance\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "# Fit the PCA instance to our model\n",
    "X_train_pca=pca.fit_transform(X_train_sc)\n",
    "X_test_pca=pca.fit_transform(X_test_sc)\n",
    "\n",
    "# Run logistic regression model with our transformed data\n",
    "pca_lr = LogisticRegression(random_state=33).fit(X_train_pca, y_train)\n",
    "\n",
    "# Accuracy appears to be the same\n",
    "pca_lr.score(X_test_pca, y_test)\n",
    "\n",
    "# Get my predicted values\n",
    "y_pred_pca = pca_lr.predict(X_test_pca)\n",
    "\n",
    "# View model performance\n",
    "print(classification_report(y_test, y_pred_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9f4d3",
   "metadata": {},
   "source": [
    "The classification report above displays that the PCA technique actually made our model worse. By removing the two features, our recall reduced to 41% and the overall accuracy reduced to 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d0d8a",
   "metadata": {},
   "source": [
    "**Second technique: Linear Discriminant Analysis (LDA)**\n",
    "\n",
    "LDA is similar to PCA in that it is another linear dimension reduction technique. They also both aim to find the features that best explain the data. PCA focuses on finding maximum variation and is unsupervised. Constrastingly, LDA is a supervised technique that focuses more on finding maximum separation between groupings. It is especially useful for classification problems with a multitude of possible outputs (as opposed to binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f13d8",
   "metadata": {},
   "source": [
    "For this technique, we want to set the number of components to classes-1. Our data are binary with two output classes, so we set our number of components to 1. Code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e12aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.85       150\n",
      "           1       0.75      0.59      0.66        81\n",
      "\n",
      "    accuracy                           0.79       231\n",
      "   macro avg       0.78      0.74      0.75       231\n",
      "weighted avg       0.78      0.79      0.78       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=33, stratify=y)\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.fit_transform(X_test)\n",
    "\n",
    "# Create the LDA instance\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "# Fit the LDA instance to our model\n",
    "X_train_lda=lda.fit_transform(X_train_sc, y_train)\n",
    "X_test_lda=lda.fit_transform(X_test_sc, y_test)\n",
    "\n",
    "# Run logistic regression model with our transformed data\n",
    "lda_lr = LogisticRegression(random_state=33).fit(X_train_lda, y_train)\n",
    "\n",
    "# Get accuracy score\n",
    "lda_lr.score(X_test_lda, y_test)\n",
    "\n",
    "# Get my predicted values\n",
    "y_pred_lda = lda_lr.predict(X_test_lda)\n",
    "\n",
    "# View model performance\n",
    "print(classification_report(y_test, y_pred_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993fc30e",
   "metadata": {},
   "source": [
    "This model has done the best so far. Out recall increased to 59% and overall accuracy increased to 79%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d992090",
   "metadata": {},
   "source": [
    "**Third technique: Isomap Embedding**\n",
    "\n",
    "Isomap embedding is a non-linear approach to dimensionality reduction. It first calculates the K-nearest neighbors for each data point. Data points that are \"neighbors\" with each other get connected. Next, the shortest path (using geodisic distance) between different nodes are calculated. Finally, the top n vectors represent the final coordinates of the newly constructed n-dimenionsal space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28c7c77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.95      0.81       150\n",
      "           1       0.73      0.23      0.36        81\n",
      "\n",
      "    accuracy                           0.70       231\n",
      "   macro avg       0.71      0.59      0.58       231\n",
      "weighted avg       0.71      0.70      0.65       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=33, stratify=y)\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.fit_transform(X_test)\n",
    "\n",
    "# Create the Isomap instance\n",
    "iso = Isomap(n_neighbors=3, n_components=5)\n",
    "\n",
    "# Fit the LDA instance to our model\n",
    "X_train_iso=iso.fit_transform(X_train_sc, y_train)\n",
    "X_test_iso=iso.fit_transform(X_test_sc, y_test)\n",
    "\n",
    "# Run logistic regression model with our transformed data\n",
    "iso_lr = LogisticRegression(random_state=33).fit(X_train_iso, y_train)\n",
    "\n",
    "# Get accuracy score\n",
    "iso_lr.score(X_test_iso, y_test)\n",
    "\n",
    "# Get my predicted values\n",
    "y_pred_iso = iso_lr.predict(X_test_iso)\n",
    "\n",
    "# View model performance\n",
    "print(classification_report(y_test, y_pred_iso))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106fc20",
   "metadata": {},
   "source": [
    "This model performed significantly worse than the other models, with a recall of 23% and an accuracy score of 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fc6de",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3e872",
   "metadata": {},
   "source": [
    "Of all the techniques above, the LDA worked best - even better than the logistic regression model on its own. This is unsurprising to me, because this technique seems to be best suited for the diabetes data set. The LDA is better, in general, for classification problems. The make the model even better, we could conduct some resampling to boost our numbers from the minority class. The imbalanced data is likely hurting the performance more than anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318ca78",
   "metadata": {},
   "source": [
    "**2. Write a function that will indicate if an inputted IPv4 address is accurate or not. IP addresses are valid if they have 4 values between 0 and 255 (inclusive), punctuated by periods.**\n",
    "\n",
    "**Input 1:**\n",
    "\n",
    "2.33.245.5\n",
    "\n",
    "**Output 1:**\n",
    "\n",
    "True\n",
    "\n",
    "**Input 2:**\n",
    "\n",
    "12.345.67.89\n",
    "\n",
    "**Output 2:**\n",
    "\n",
    "False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84dae445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that takes in an IPv4 address as an argument\n",
    "def IPv4(IP):\n",
    "    try:\n",
    "        # Store results\n",
    "        result = []\n",
    "        # Ensure the IPv4 address is a string\n",
    "        if isinstance(IP, str):\n",
    "            # Separate the string by the delimiter ('.')\n",
    "            split_IP = IP.split('.')\n",
    "            # Iterate through split_IP\n",
    "            for i in split_IP:\n",
    "                # Convert i to integer\n",
    "                i = int(i)\n",
    "                # Check to see if individual numbers are between 0 and 256\n",
    "                if i in range(0,256):\n",
    "                    result.append(True)\n",
    "                else:\n",
    "                    result.append(False)\n",
    "        # Check to ensure there are only 4 values in the result list\n",
    "        if len(result) == 4:\n",
    "            # Print a boolean expression to check whether ALL items in result are 'true'\n",
    "            # If there are any false, it will return false\n",
    "            print(all(r == True for r in result))\n",
    "        else:\n",
    "            print('False')\n",
    "        \n",
    "    except ValueError:\n",
    "        print('Please enter valid IP address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71370f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Input 1\n",
    "IPv4('2.33.245.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772dee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Input 2\n",
    "IPv4('12.345.67.89')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a319a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Input 3 (5 values instead of 4, but all within range)\n",
    "IPv4('2.33.245.5.3.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6bd2a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "IPv4(2) # random numer, invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a575e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter valid IP address\n"
     ]
    }
   ],
   "source": [
    "IPv4('banana')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
